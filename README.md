# Egyptian Currency Classification using CNN

<div align="center">

### ğŸ“ Multimedia Deep Learning Course Project
**Supervised by: Dr. Eman Gouda**

---

### ğŸ‘¥ Team Members

| Name | ID |
|------|-----|
| Karim Wael Balbaa | 241482823 |
| Moustafa Ahmed Moustafa Abdelmoneim | 241477284 |
| Mathew Moussa Halim | 241477219 |
| Ali Khaled Ali Kotb Shaaban | 241477554 |
| Ahmed Ibrahim Amin El Refai | 241496423 |
| Ezzeldin Ashraf Mohamed | 241407315 |

---

</div>

## ğŸ“– Project Overview

A deep learning project that classifies Egyptian banknotes (2023 new currency) using a **custom Convolutional Neural Network (CNN) built from scratch** with Keras/TensorFlow. 

**Key Achievement**: ğŸ¯ **96.55% Test Accuracy** (Exceeds 93% target requirement)

**Important**: âš ï¸ **No transfer learning or pre-trained models were used** (as per project requirements). The entire CNN architecture was designed and trained from scratch.

### ğŸ”— Dataset Source
[Egyptian New Currency 2023 - Kaggle](https://www.kaggle.com/datasets/belalsafy/egyptian-new-currency-2023)

---

## ğŸ“‹ Table of Contents
- [Problem Statement](#problem-statement)
- [Dataset](#dataset)
- [CNN Architecture](#cnn-architecture)
- [Training Configuration](#training-configuration)
- [Results](#results)
  - [Training Curves](#training-curves)
  - [Test Results & Metrics](#test-results--metrics)
  - [Confusion Matrix](#confusion-matrix)
  - [Example Predictions](#example-predictions)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Repository Structure](#repository-structure)

---

## Problem Statement

**Objective**: Build a CNN model from scratch to classify Egyptian banknotes into their respective denominations.

**Challenge**: 
- Classify 9 different Egyptian currency denominations: **1, 5, 10, 20, 50, 100, 200 EGP**
- Handle newly issued notes: **"10 (new)"** and **"20 (new)"** alongside older versions
- Deal with class imbalance (Class "1" has only 60 training samples vs. 346 for "20 (new)")
- Work with limited data (~2,600 training images) requiring strong regularization and augmentation
- Achieve real-world robustness across varying lighting, angles, and image quality

**Why This Matters**: Automated currency classification is useful for:
- Banking automation and ATM systems
- Retail point-of-sale verification
- Mobile payment apps with visual verification
- Accessibility tools for visually impaired users

---


## ğŸ“Š Dataset

### Overview

<div align="center">

| Metric | Value |
|--------|-------|
| **Total Images** | 3,687 |
| **Training Set** | 2,637 (71.5%) |
| **Validation Set** | 760 (20.6%) |
| **Test Set** | 290 (7.9%) |
| **Classes** | 9 denominations |
| **Image Format** | RGB, resized to 96Ã—96 pixels |
| **Source** | [Kaggle Dataset](https://www.kaggle.com/datasets/belalsafy/egyptian-new-currency-2023) |

</div>

### ğŸ’µ Class Distribution (Training Set)

<div align="center">

| Class | Count | Percentage | Class Weight | Status |
|-------|-------|------------|--------------|---------|
| 1 EGP | 60 | 2.3% | 4.88 | âš ï¸ Underrepresented |
| 5 EGP | 334 | 12.7% | 0.88 | âœ… Balanced |
| 10 EGP (old) | 315 | 11.9% | 0.93 | âœ… Balanced |
| 10 EGP (new) | 317 | 12.0% | 0.92 | âœ… Balanced |
| 20 EGP (old) | 322 | 12.2% | 0.91 | âœ… Balanced |
| 20 EGP (new) | 346 | 13.1% | 0.85 | âœ… Balanced |
| 50 EGP | 315 | 11.9% | 0.93 | âœ… Balanced |
| 100 EGP | 315 | 11.9% | 0.93 | âœ… Balanced |
| 200 EGP | 313 | 11.9% | 0.94 | âœ… Balanced |

</div>

**Note**: Class weights were computed using `sklearn.utils.class_weight.compute_class_weight('balanced', ...)` and applied during training to handle the significant imbalance, particularly for the 1 EGP class with only 60 samples.

### ğŸ”„ Data Augmentation Strategy

To improve model generalization with limited data, **aggressive augmentation** was applied exclusively to the training set:

<div align="center">

| Technique | Parameters | Purpose |
|-----------|------------|---------|
| **Rescaling** | `1./255` | Normalize pixels to [0, 1] |
| **Rotation** | `Â±20Â°` | Simulate different viewing angles |
| **Width/Height Shift** | `Â±15%` | Handle position variance |
| **Shear Transform** | `Â±15%` | Simulate perspective changes |
| **Zoom** | `Â±20%` | Account for distance variance |
| **Brightness** | `[0.7, 1.3]` | Simulate different lighting conditions |
| **Channel Shift** | `Â±30` | RGB color perturbation |
| **Horizontal Flip** | `False` | âš ï¸ **Disabled** - Currency orientation matters! |
| **Fill Mode** | `nearest` | Border pixel handling |

</div>

**Validation/Test Sets**: Only rescaling (`1./255`) applied - **no augmentation** to ensure fair evaluation.

### ğŸ“¸ Sample Images from Dataset

![Sample Images](sample_converted_8_0.png)
<div align="center"><i>Representative samples from each of the 9 currency classes</i></div>

![Augmented Samples](sample_converted_14_0.png)
<div align="center"><i>Examples of augmented training images showing various transformations</i></div>

---


## ğŸ—ï¸ CNN Architecture

### Design Philosophy

Built a **custom CNN from scratch** (no transfer learning) with strong regularization techniques to prevent overfitting on the limited dataset (~2,600 training images). The architecture follows modern best practices:

âœ… Gradual feature hierarchy (32 â†’ 64 â†’ 128 filters)  
âœ… Batch normalization for training stability  
âœ… Progressive dropout for regularization  
âœ… L2 weight regularization  
âœ… Compact classification head to prevent memorization  

### ğŸ”§ Architecture Details

```
INPUT: (96, 96, 3) RGB images
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BLOCK 1: Low-Level Feature Extraction              â”‚
â”‚ Purpose: Detect edges, colors, basic shapes        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv2D(32 filters, 3Ã—3, ReLU, L2=0.001)            â”‚  â† Edge detection
â”‚ BatchNormalization()                                â”‚  â† Stabilize training
â”‚ Conv2D(32 filters, 3Ã—3, ReLU, L2=0.001)            â”‚  â† Refine features
â”‚ BatchNormalization()                                â”‚
â”‚ MaxPooling2D(2Ã—2)                                   â”‚  â† Downsample: 96Ã—96 â†’ 48Ã—48
â”‚ Dropout(0.3)                                        â”‚  â† 30% regularization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BLOCK 2: Mid-Level Feature Extraction              â”‚
â”‚ Purpose: Detect textures, patterns, currency marks â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv2D(64 filters, 3Ã—3, ReLU, L2=0.001)            â”‚  â† Texture detection
â”‚ BatchNormalization()                                â”‚
â”‚ Conv2D(64 filters, 3Ã—3, ReLU, L2=0.001)            â”‚  â† Complex patterns
â”‚ BatchNormalization()                                â”‚
â”‚ MaxPooling2D(2Ã—2)                                   â”‚  â† Downsample: 48Ã—48 â†’ 24Ã—24
â”‚ Dropout(0.3)                                        â”‚  â† 30% regularization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BLOCK 3: High-Level Feature Extraction             â”‚
â”‚ Purpose: Recognize currency-specific features      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv2D(128 filters, 3Ã—3, ReLU, L2=0.001)           â”‚  â† Currency identifiers
â”‚ BatchNormalization()                                â”‚
â”‚ Conv2D(128 filters, 3Ã—3, ReLU, L2=0.001)           â”‚  â† Denomination features
â”‚ BatchNormalization()                                â”‚
â”‚ MaxPooling2D(2Ã—2)                                   â”‚  â† Downsample: 24Ã—24 â†’ 12Ã—12
â”‚ Dropout(0.4)                                        â”‚  â† 40% regularization (stronger)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLASSIFICATION HEAD                                 â”‚
â”‚ Purpose: Map features to class probabilities       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Flatten()                                           â”‚  â† 12Ã—12Ã—128 = 18,432 features
â”‚ Dense(256 neurons, ReLU, L2=0.001)                 â”‚  â† Compact representation
â”‚ BatchNormalization()                                â”‚
â”‚ Dropout(0.5)                                        â”‚  â† 50% regularization (strongest)
â”‚ Dense(9 neurons, Softmax)                          â”‚  â† 9 class probabilities
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
OUTPUT: [pâ‚, pâ‚‚, ..., pâ‚‰] where Î£páµ¢ = 1.0
```

### ğŸ“Š Model Statistics

<div align="center">

| Metric | Value |
|--------|-------|
| **Total Parameters** | 5,010,985 |
| **Trainable Parameters** | 5,009,577 |
| **Non-trainable Parameters** | 1,408 (BatchNorm stats) |
| **Model Size** | ~19.12 MB |
| **Input Shape** | (96, 96, 3) |
| **Output Shape** | (9,) - Class probabilities |

</div>

### ğŸ¯ Key Design Decisions

<div align="center">

| Decision | Rationale |
|----------|-----------|
| **32 â†’ 64 â†’ 128 filters** | Gradual increase mimics natural visual hierarchy (edges â†’ textures â†’ objects) |
| **L2 Regularization (0.001)** | Penalizes large weights, preventing overfitting without being too aggressive |
| **Batch Normalization** | Normalizes activations, enabling higher learning rates and faster convergence |
| **Progressive Dropout (0.3 â†’ 0.5)** | Stronger regularization in deeper layers where overfitting risk is higher |
| **Dense(256) not Dense(512)** | Smaller fully connected layer prevents memorization on limited data |
| **No Horizontal Flip** | Currency orientation is semantically important (can't flip money!) |
| **3Ã—3 Convolutions** | Standard receptive field, computationally efficient |
| **Adam Optimizer** | Adaptive learning rates work well for this problem complexity |

</div>

### ğŸ“ Receptive Field Analysis

- **Block 1 output**: Captures ~7Ã—7 pixel regions (local features)
- **Block 2 output**: Captures ~15Ã—15 pixel regions (mid-level features)  
- **Block 3 output**: Captures ~31Ã—31 pixel regions (global features)
- **Final layer**: Full 96Ã—96 image context via global pooling

---


## âš™ï¸ Training Configuration

### ğŸ›ï¸ Hyperparameters

<div align="center">

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **Image Size** | 96Ã—96 | Balance between speed and detail retention |
| **Batch Size** | 32 | Small batches for better generalization |
| **Epochs** | 100 | With early stopping to prevent overfitting |
| **Optimizer** | Adam | Adaptive learning rate for faster convergence |
| **Initial LR** | 5Ã—10â»â´ (0.0005) | Conservative to avoid overshooting |
| **Loss Function** | Categorical Crossentropy | Standard for multi-class classification |
| **Metrics** | Accuracy | Primary evaluation metric |

</div>

### ğŸ“ˆ Training Strategy

<div align="center">

| Strategy Component | Configuration | Purpose |
|-------------------|---------------|---------|
| **Class Weights** | Computed via `sklearn` | Handle severe imbalance (1 EGP: 4.88 vs 20 new: 0.85) |
| **Learning Rate Reduction** | `ReduceLROnPlateau` | Reduce LR by 50% if val_loss plateaus for 8 epochs |
| **Minimum LR** | 1Ã—10â»â· | Prevent LR from becoming too small |
| **Early Stopping** | 25 epoch patience | Stop if no val_loss improvement, restore best weights |
| **Model Checkpointing** | Monitor `val_accuracy` | Save only the best model during training |
| **Random Seeds** | `np.random.seed(42)`, `tf.random.set_seed(42)` | Ensure reproducibility |

</div>

### ğŸ”„ Training Pipeline

```
1. Load Data
   â”œâ”€ Training: 2,637 images â†’ Apply augmentation
   â”œâ”€ Validation: 760 images â†’ No augmentation
   â””â”€ Test: 290 images â†’ No augmentation

2. Compute Class Weights
   â””â”€ Handle imbalance: 1 EGP gets 4.88Ã— weight

3. Build Model
   â””â”€ Custom CNN with 5M parameters

4. Compile Model
   â”œâ”€ Optimizer: Adam(lr=5e-4)
   â”œâ”€ Loss: Categorical Crossentropy
   â””â”€ Metrics: Accuracy

5. Train with Callbacks
   â”œâ”€ ReduceLROnPlateau â†’ Adaptive learning rate
   â”œâ”€ EarlyStopping â†’ Prevent overfitting
   â””â”€ ModelCheckpoint â†’ Save best model

6. Evaluate
   â”œâ”€ Validation Set: 96.97% accuracy
   â””â”€ Test Set: 96.55% accuracy âœ…
```

### ğŸ“Š Training Statistics

<div align="center">

| Metric | Value |
|--------|-------|
| **Total Epochs Run** | 100 |
| **Best Epoch** | 93 (restored by EarlyStopping) |
| **Steps per Epoch** | 83 (ceil(2637/32)) |
| **Validation Steps** | 24 (ceil(760/32)) |
| **LR Reductions** | 2 times (epochs 43 and 85) |
| **Final Training Accuracy** | 97.91% |
| **Best Validation Accuracy** | 97.24% |
| **Training Time** | ~2-3 hours (CPU) / ~30-40 min (GPU) |

</div>

### ğŸ¯ Reproducibility Checklist

- âœ… Random seeds fixed (`42` for both NumPy and TensorFlow)
- âœ… Same data splits used consistently
- âœ… Deterministic augmentation pipeline
- âœ… Fixed batch size and training order
- âœ… Model architecture fully specified
- âœ… All hyperparameters documented

---


## ğŸ“Š Results

### ğŸ“ˆ Training Curves

The model was trained for 100 epochs with early stopping. The training curves demonstrate excellent learning dynamics with proper regularization:

<div align="center">

![Training Curves](sample_converted_26_0.png)

</div>

**Key Observations**:

<div align="center">

| Aspect | Observation |
|--------|-------------|
| **Training Accuracy** | Reached ~98% by epoch 100 |
| **Validation Accuracy** | Peaked at **97.24%** around epoch 93 |
| **Learning Rate Reductions** | 2 times (epochs 43 and 85) when loss plateaued |
| **Overfitting** | âœ… **Minimal** - Train/val gap remained reasonable |
| **Early Stopping** | Restored weights from epoch 93 (best validation) |
| **Convergence** | Smooth and stable throughout training |

</div>

### ğŸ¯ Test Results & Metrics

Final evaluation on the **held-out test set** (290 images):

<div align="center">

| Metric | Training | Validation | **Test** |
|--------|----------|------------|----------|
| **Accuracy** | 97.91% | 96.97% | **96.55%** âœ… |
| **Loss** | - | 0.5459 | **0.5356** |
| **Target** | - | - | **â‰¥93%** |
| **Status** | - | - | **âœ… EXCEEDED** |

</div>

**ğŸ‰ SUCCESS**: Exceeded the 93% accuracy target requirement by **3.55 percentage points**!

### ğŸ“‹ Classification Report (Validation Set)

<div align="center">

```
                  precision    recall  f1-score   support

           1       1.00      0.95      0.97        20
          10       1.00      0.94      0.97        80
    10 (new)       1.00      0.98      0.99       130
         100       0.96      0.97      0.97        80
          20       0.94      0.97      0.96        80
    20 (new)       1.00      0.99      1.00       130
         200       0.99      0.99      0.99        80
           5       0.90      0.97      0.93        80
          50       0.94      0.93      0.93        80

    accuracy                           0.97       760
   macro avg       0.97      0.97      0.97       760
weighted avg       0.97      0.97      0.97       760
```

</div>

### ğŸ† Per-Class Performance Analysis

<div align="center">

| Class | Accuracy | Correct/Total | Performance |
|-------|----------|---------------|-------------|
| **20 (new)** | 99.2% | 129/130 | ğŸ¥‡ **Excellent** |
| **200 EGP** | 98.8% | 79/80 | ğŸ¥ˆ **Excellent** |
| **10 (new)** | 97.7% | 127/130 | ğŸ¥‰ **Excellent** |
| **20 EGP** | 97.5% | 78/80 | âœ… **Very Good** |
| **100 EGP** | 97.5% | 78/80 | âœ… **Very Good** |
| **5 EGP** | 97.5% | 78/80 | âœ… **Very Good** |
| **1 EGP** | 95.0% | 19/20 | âœ… **Good** |
| **10 EGP** | 93.8% | 75/80 | âš ï¸ **Challenging** |
| **50 EGP** | 92.5% | 74/80 | âš ï¸ **Challenging** |

</div>

**Analysis**:
- ğŸ¯ **Best performers**: New currency designs (10 new, 20 new) - clearer features
- ğŸ’ª **Strong performance**: Most classes >95% accuracy despite limited training data
- ğŸ” **Challenging cases**: 50 EGP and old 10 EGP - likely due to similar visual features
- ğŸ… **Impressive**: 1 EGP achieves 95% despite having only 60 training samples!

### ğŸ—ºï¸ Confusion Matrix

<div align="center">

![Confusion Matrix](sample_converted_32_0.png)

</div>

**Key Insights**:

- âœ… **Strong diagonal dominance** - Most predictions are correct
- ğŸ”€ **Main confusion**: 50 EGP â†” similar denominations (occasional mix-ups)
- ğŸ¯ **Excellent separation**: Old vs. new 10/20 notes (different designs help model distinguish)
- ğŸ’¯ **Perfect precision**: Classes 1, 10, 10 (new), and 20 (new) - no false positives
- ğŸ“ **Class imbalance handled well**: 1 EGP performs surprisingly well despite limited data

### ğŸ–¼ï¸ Example Predictions on Test Images

<div align="center">

![Example Predictions](sample_converted_34_1.png)

</div>

**Prediction Analysis**:

- ğŸŸ¢ **Green titles** = Correct predictions with high confidence scores (typically >90%)
- ğŸ”´ **Red titles** = Misclassifications (very rare in test set)
- ğŸ’ª **Robustness**: Model handles various:
  - Image qualities (low to high resolution)
  - Viewing angles (slight rotations and perspectives)
  - Lighting conditions (bright, dim, natural, artificial)
  - Image backgrounds (clean and cluttered)
- ğŸ¯ **Confidence**: Most correct predictions show >90% confidence
- ğŸ”¬ **Real-world ready**: Performs well on unseen images with natural variations

### ğŸ“Š Sample Predictions Breakdown

<div align="center">

| Image Quality | Angle | Lighting | Prediction | Confidence |
|--------------|-------|----------|------------|------------|
| High | Frontal | Good | âœ… Correct | 95-99% |
| Medium | Slight tilt | Variable | âœ… Correct | 85-95% |
| Low | Rotated | Poor | âš ï¸ Mixed | 70-90% |

</div>

---


## Quick Start

### ğŸ’» System Requirements

- **Python**: 3.10 or 3.11 (recommended)
- **RAM**: Minimum 8GB
- **GPU**: Optional (CUDA-compatible GPU recommended for faster training)
- **Storage**: ~2GB for dataset and models

### ğŸ“¦ Installation

#### For All Platforms (Windows/Linux/macOS)

```bash
# Clone the repository
git clone https://github.com/KarimSIM2024/Egyptian_New_Currency.git
cd Egyptian_New_Currency

# Create virtual environment
python3 -m venv .venv

# Activate virtual environment
# On macOS/Linux:
source .venv/bin/activate

# On Windows (Command Prompt):
.venv\Scripts\activate.bat

# On Windows (PowerShell):
.venv\Scripts\Activate.ps1

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

#### Platform-Specific TensorFlow Installation

**macOS (Apple Silicon - M1/M2/M3)**:
```bash
pip install tensorflow-macos tensorflow-metal
```

**Windows/Linux with NVIDIA GPU**:
```bash
# For CUDA-enabled GPU acceleration
pip install tensorflow[and-cuda]
```

**CPU-Only (All Platforms)**:
```bash
# Already included in requirements.txt
pip install tensorflow
```

### âš¡ Quick Inference

Test the trained model on a single image:

```python
import tensorflow as tf
import numpy as np
from PIL import Image

# Load the trained model
model = tf.keras.models.load_model('best_currency_model.keras')

# Prepare image
img = Image.open('your_image.jpg').convert('RGB').resize((96, 96))
arr = np.array(img) / 255.0
arr = np.expand_dims(arr, axis=0)

# Predict
class_names = ['1', '10', '10 (new)', '100', '20', '20 (new)', '200', '5', '50']
probs = model.predict(arr)[0]
pred_class = class_names[np.argmax(probs)]
confidence = np.max(probs)

print(f'ğŸ’° Predicted: {pred_class} EGP (Confidence: {confidence*100:.1f}%)')
```

### ğŸš€ Run Training Notebook

```bash
# Install Jupyter if not already installed
pip install jupyter

# Launch Jupyter Notebook
jupyter notebook sample.ipynb
```

---

## Usage

### Training from Scratch

Open and run `sample.ipynb` in Jupyter:

```bash
jupyter notebook sample.ipynb
```

Or use the provided training script skeleton:

```python
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Configuration
IMG_SIZE = (96, 96)
BATCH_SIZE = 32
NUM_EPOCHS = 100

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.15,
    zoom_range=0.2,
    brightness_range=[0.7, 1.3],
    channel_shift_range=30,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1.0/255)

train_gen = train_datagen.flow_from_directory(
    'dataset/train', target_size=IMG_SIZE, batch_size=BATCH_SIZE, 
    class_mode='categorical', shuffle=True
)

val_gen = val_datagen.flow_from_directory(
    'dataset/valid', target_size=IMG_SIZE, batch_size=BATCH_SIZE,
    class_mode='categorical', shuffle=False
)

# Build model
reg = regularizers.l2(0.001)
model = models.Sequential([
    layers.Input(shape=IMG_SIZE + (3,)),
    # Block 1
    layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.3),
    # Block 2
    layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.3),
    # Block 3
    layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.4),
    # Classification
    layers.Flatten(),
    layers.Dense(256, activation='relu', kernel_regularizer=reg),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(9, activation='softmax')
])

# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(5e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Compute class weights
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(train_gen.classes),
    y=train_gen.classes
)
class_weight_dict = dict(enumerate(class_weights))

# Callbacks
callbacks = [
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=25, restore_best_weights=True, verbose=1
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'best_currency_model.keras', monitor='val_accuracy', 
        save_best_only=True, verbose=1
    )
]

# Train
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=NUM_EPOCHS,
    callbacks=callbacks,
    class_weight=class_weight_dict
)
```

### Evaluation
### Evaluation

Evaluate the model on test data:

```python
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

model = tf.keras.models.load_model('best_currency_model.keras')

test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\
    .flow_from_directory('dataset/test', target_size=(96, 96), 
                        batch_size=32, class_mode='categorical', shuffle=False)

# Evaluate
loss, acc = model.evaluate(test_gen)
print(f'Test Accuracy: {acc*100:.2f}%')

# Predictions and metrics
y_true = test_gen.classes
y_pred = np.argmax(model.predict(test_gen), axis=1)

print(classification_report(y_true, y_pred))
print(confusion_matrix(y_true, y_pred))
```

### Batch Inference

Process multiple images:

```python
import os
from PIL import Image
import numpy as np
import tensorflow as tf

model = tf.keras.models.load_model('best_currency_model.keras')
class_names = ['1', '10', '10 (new)', '100', '20', '20 (new)', '200', '5', '50']

folder = 'path/to/images'
for fname in os.listdir(folder):
    if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
        img = Image.open(os.path.join(folder, fname)).convert('RGB').resize((96, 96))
        arr = np.expand_dims(np.array(img) / 255.0, axis=0)
        pred_idx = np.argmax(model.predict(arr, verbose=0))
        conf = np.max(model.predict(arr, verbose=0))
        print(f'{fname}: {class_names[pred_idx]} ({conf*100:.1f}%)')
```

---

## Repository Structure

```
Egyptian_New_Currency/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ sample.ipynb                       # Training notebook
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ best_currency_model.keras          # Best trained model
â”œâ”€â”€ egyptian_currency_model.keras      # Additional checkpoint
â”œâ”€â”€ model.png                          # Architecture diagram
â”œâ”€â”€ sample_converted_*.png             # Result visualizations
â”œâ”€â”€ Course Project Description.pdf     # Project brief
â”œâ”€â”€ dataset/                           # Dataset root
â”‚   â”œâ”€â”€ train/                        # Training images (2,637)
â”‚   â”‚   â”œâ”€â”€ 1/
â”‚   â”‚   â”œâ”€â”€ 5/
â”‚   â”‚   â”œâ”€â”€ 10/
â”‚   â”‚   â”œâ”€â”€ 10 (new)/
â”‚   â”‚   â”œâ”€â”€ 20/
â”‚   â”‚   â”œâ”€â”€ 20 (new)/
â”‚   â”‚   â”œâ”€â”€ 50/
â”‚   â”‚   â”œâ”€â”€ 100/
â”‚   â”‚   â””â”€â”€ 200/
â”‚   â”œâ”€â”€ valid/                        # Validation images (760)
â”‚   â””â”€â”€ test/                         # Test images (290)
â””â”€â”€ models/                            # Additional model exports
```

---

## Key Takeaways

âœ… **Custom CNN built from scratch** - No transfer learning or pretrained models  
âœ… **96.55% test accuracy** - Exceeds 93% target  
âœ… **Robust to class imbalance** - Weighted loss and data augmentation  
âœ… **Strong regularization** - L2, BatchNorm, Dropout prevent overfitting  
âœ… **Real-world ready** - Handles varying lighting, angles, and image quality  

---

## Future Improvements

- [ ] Export to TensorFlow Lite for mobile deployment
- [ ] Implement real-time video classification
- [ ] Add explainability (Grad-CAM visualizations)
- [ ] Expand dataset for rare denominations (especially 1 EGP)
- [ ] Test on degraded/worn currency notes

---

## ğŸ”¬ Data Preprocessing Pipeline

Our preprocessing pipeline ensures consistent input quality and optimal model performance through several key steps:

### 1ï¸âƒ£ Image Loading and Format Standardization

```python
# Using Keras ImageDataGenerator for efficient data loading
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# All images are loaded and automatically:
# - Converted to RGB format (3 channels)
# - Resized to 96Ã—96 pixels (fixed input size)
# - Loaded in batches of 32 for memory efficiency
```

**Why 96Ã—96?**
- Balance between computational efficiency and detail preservation
- Smaller than typical 224Ã—224 to speed up training
- Sufficient resolution to capture currency features (text, symbols, colors)

### 2ï¸âƒ£ Pixel Normalization

```python
# Rescale pixel values from [0, 255] to [0, 1]
rescale=1./255
```

**Purpose**:
- Neural networks train better with normalized inputs
- Prevents gradient explosion/vanishing
- Ensures all features are on the same scale
- Faster convergence during training

### 3ï¸âƒ£ Training Set Augmentation (Real-time)

Applied **only to training data** during training (on-the-fly):

<div align="center">

| Augmentation | Implementation | Effect |
|--------------|----------------|--------|
| **Rotation** | `rotation_range=20` | Random rotation Â±20 degrees |
| **Width Shift** | `width_shift_range=0.15` | Horizontal translation Â±15% |
| **Height Shift** | `height_shift_range=0.15` | Vertical translation Â±15% |
| **Shear** | `shear_range=0.15` | Shearing transformation Â±15% |
| **Zoom** | `zoom_range=0.2` | Zoom in/out Â±20% |
| **Brightness** | `brightness_range=[0.7, 1.3]` | Brightness variation 70%-130% |
| **Channel Shift** | `channel_shift_range=30` | RGB channel intensity shift Â±30 |

</div>

**Augmentation Benefits**:
- ğŸ“ˆ **Increases effective training data** from 2,637 to effectively infinite variations
- ğŸ¯ **Prevents overfitting** by exposing model to diverse examples
- ğŸ’ª **Improves generalization** to real-world scenarios
- ğŸŒ **Simulates real conditions**: different angles, lighting, distances

**What We DON'T Do**:
- âŒ **No horizontal flip** - Currency orientation is meaningful (numbers/text direction matters)
- âŒ **No vertical flip** - Same reason as above
- âŒ **No augmentation on validation/test** - Ensures fair, consistent evaluation

### 4ï¸âƒ£ Class Weight Computation

```python
from sklearn.utils.class_weight import compute_class_weight

# Compute balanced weights for imbalanced classes
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
```

**Why Class Weights?**
- Class "1 EGP" has only **60 samples** vs "20 (new)" with **346 samples**
- Without weights: model would ignore rare classes
- With weights: Each "1 EGP" sample counts **5.75Ã— more** (4.88/0.85) than "20 (new)"
- Result: Model learns all classes equally well

### 5ï¸âƒ£ Batch Processing

```python
# Data is loaded in batches of 32 images
BATCH_SIZE = 32

# Training: 2637 images â†’ 83 batches per epoch
# Validation: 760 images â†’ 24 batches per epoch
# Test: 290 images â†’ 10 batches
```

**Benefits**:
- ğŸ’¾ **Memory efficient** - Don't load all images at once
- âš¡ **Faster training** - GPU processes batches in parallel
- ğŸ“Š **Better gradients** - Batch normalization statistics

### 6ï¸âƒ£ Label Encoding

```python
# One-hot encoding for categorical classification
class_mode='categorical'

# Example: "10 (new)" â†’ [0, 0, 1, 0, 0, 0, 0, 0, 0]
#          Class index 2 is activated
```

**Preprocessing Summary**:

```
Raw Image (Variable size, 0-255 RGB)
           â†“
[1] Load & Resize â†’ 96Ã—96Ã—3
           â†“
[2] Normalize â†’ Pixels Ã· 255 â†’ [0, 1]
           â†“
[3] Augment (if training) â†’ Random transforms
           â†“
[4] Batch â†’ Group 32 images
           â†“
[5] One-hot encode labels â†’ 9-class vector
           â†“
Ready for CNN Input!
```

---

## ğŸ“‚ Project Repository

**GitHub**: [KarimSIM2024/Egyptian_New_Currency](https://github.com/KarimSIM2024/Egyptian_New_Currency)

---

## License

This project is licensed under the MIT License.

---

**Developed with â¤ï¸ for Multimedia Deep Learning Course**
